---
title: "Bayesian Analysis of Esports Teams' Perfomances"
subtitle: "Aalto University"
author: 
- Ekaterina Marchenko \and
- Georgios Karakasidis \and
- Rongzhi Liu
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
editor_options: 
  chunk_output_type: console
bibliography: references.bibtex
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rstan)
library(knitr)
library(dplyr)
```

# Introduction

In this report, we present a Bayesian Analysis approach for evaluating team performances for the esport *Dota 2*. This topic was chosen due to the personal interest of our team members in esports. We formed the group based on this and all of us were familiar with either Dota 2 or League of Legends. 

Before describing the problem more thouroughly, we would like to provide a brief insight into what we are trying to achieve. Dota 2 is a MOBA game and an esports discipline. Esports have a similar structure as traditional sports, where there are professional seasons which roughly equal to a year, and these are finished with a main tournament (in our case: *The International (TI)*). 

The tournament itself consists of two stages: qualifications and main event. The latter is also split into two parts: group stage, and play-offs. During the group stage, teams compete in order to have a better starting position in the play-off stage. For our analysis, we used the data from the last TI where the best teams in the world competed.

The professional Dota 2 scene is divided into different regions (e.g. Europe, CIS, China, etc). Teams from these regions participate in qualifications separately, and the very first goal of the team is to become the best within their region. As a result, teams from one region play with each other much more often, and it can influence the way they perform. Hence, we also used information about the region of the teams while designing and implementing our models.

Our aim is to find a way to predict the match result based on the performances of the competing teams (the way performances are computed is discussed later). In other words, we propose a way to make the analysis of the previous matches in which teams participated, and take those results to predict how they would perform in the future.

# Data and Preprocessing

In our analysis, we decided to use matches from **The International 2019** Tournament, the results of which are available online through some APIs. The data was gathered with the Python programming language in two steps. At first, match IDs were collected from [DotaBuff](https://www.dotabuff.com/esports/events/284-ti9-group-main). Afterwards, we used the obtained IDs in order to acquire detailed information about each match. For the latter we used the [OpenDota API](https://docs.opendota.com/#tag/matches).

For our analysis, we used the data from the group stage of the tournament as each team played the same amount of matches. On the other hand, for the posterior predictive checks (which are further discussed in the [Posterior Predictive Checks](#posterior-predictive-checks) section), we made predictions for the play-off stage of TI9. 

The next step was to define a new metric in order to quantify the teams' performances. To do this we used a mixture of different measures/scores which we then normalized and used its sum. The initial features can be seen below:

```{r var_table, echo=FALSE}
variable <- c("radiant_score", "dire_score", "radiant_xp_adv", "radiant_gold_adv", 
              "hero_damage", "hero_healing", "obs_placed", "kda")
description <- c("Final score for the Radiant team(number of kills on Radiant)", 
                 "Final score for the Dire team (number of kills on Dire)", 
                 "Array of the Radiant experience advantage at each \\
                     minute in the game. A negative number means that \\
                     Radiant is behind, and thus it is their experience disadvantage.",
                 "Array of the Radiant gold advantage at each minute in the game. \\
                     A negative number means that Radiant is behind, and thus it is \\
                     their gold disadvantage.", "Hero Damage Dealt (user specific)", 
                 "Hero Healing Done (user specific)", 
                 "Total number of observer wards placed (user specific)", 
                 "kda (ratio of kills/deaths/assists) (user specific)")

knitr::kable(data.frame(variable, description), format = 'pipe', padding=100,
             col.names = c("Variable Names", "Description"))
```

These features were extracted for each match and contain information about both of the teams. Since each $Dota 2$ game consists of $5$ players, this means that we end up with $10$ data points for each match. 

As you can see from the table above, the first $4$ features have to do with team-wide statistics while the rest $4$ have to do with user-specific statistics since they evaluate the performance of each player separately. Our goal is to find a way to combine these features so that the result will be a performance score for each team, for each match. 

Our approach to this can be described by the following equation which assigns a score to each team for each match:
$$
R_{score} = score\_gap + exp\_advantage + gold\_advantage + 
$$
$$
\sum_{\forall p \in P_{R}} hero\_damage(p) + healing\_done(p) + kda(p) + wards\_placed(p)
$$

where $R_{score}$ denotes the radiant team score, and $P_R$ is the set of all radiant players on the specific match. The same was also applied for the dire team.

*NOTE: This equation is over-simplified since it does not take normalization into account (having all values in the same range).*

Each of these functions is described below:
```{r var_table_matches, echo=FALSE}
match_vars <- c("score_gap", "exp_advantage", "gold_advantage", "hero_damage", 
              "healing_done", "kda", "wards_placed")
match_vars_desc <- c("Radiant score minus Dire score (so it can also be negative).", 
                 "A weighted sum of the experience advantage that Radiant had during \\
                     the game (can also be negative). We are giving bigger weights \\
                     to the scores at the end of the game since they are more \\
                     informative about the final result.", 
                 "A weighted sum of the gold advantage that Radiant had during \\
                     the game (can also be negative). We are giving bigger weights \\
                     to the scores at the end of the game since they are more \\
                     informative about the final result.", 
                 "Simplistic Approach: The difference of the sum of the damage dealt by \\
                 all Radiant players when compared to Dire players.", 
                 "Simplistic Approach: The difference of the sum of the healing that \\
                 was done by all Radiant players when compared to Dire players.", 
                 "Simplistic Approach: The difference of the sum of kdas of \\
                 all Radiant players when compared to Dire players.", 
                 "Simplistic Approach: The difference of the sum of the number of \\
                 observation wards that were placed by all Radiant players when \\
                 compared to Dire players.")

knitr::kable(data.frame(match_vars, match_vars_desc), format = 'pipe', padding=100,
             col.names = c("Variable Names", "Description"))
```

As you can see, a rather simplistic approach was used to combine the data. As a normalization technique we are currently using MinMax scaling which does not completely distort our values and so it will keep the following notion:

- Large performance scores should probably result in a Radiant win (not always).
- Smaller scores (negative ones) should point us to the direction that Dire will most probably win.

For more information about the implementation you may check the [data transformation appendix](#data-transformation). An important note here is that, for each match, the $score\_gap$, $exp\_gap$ and $gold\_advantage$ columns for the Dire team are multiplied by $-1$ so that they will be positive in case the Dire has positive statistics (and negative otherwise). 

# Problem Solving

By preprocessing our data we managed to get a single scoring measurement for each team. Our next step is to build a model on top of these transformed data and try to predict/sample future scores (future performances). Our approach invloved two models which are going to be described below. For the implementation we used the RStan [@rstan] package of R.

At first, we are going to represent our data as a 2-dimensional array which will contain information about each team. The rows will denote the matches and the columns will denote the teams, while the content of each array cell will be the above computed performance scores.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(rstan)
library(knitr)
library(dplyr)
library(loo)

# read the match performance of each team in group stage
# have make the data to look like factory data
data <- read.csv(file = './data/match_performance_group_region.csv')
```



Below, we are going to describe two model-architecture approaches, hierarchical and separate models. In the first case, under the hierarchical normal model, we assume that our data (Dota 2 matches per team for each region) are independently normally distributed within a certain number of groups (which is equal to the number of teams that each region has).Here, all groups share a common variance $\sigma^2$ while they have different (team-specific) means $\theta_j$ [@gelman2013bayesian]. We also assume that the means follow a certain distribution (disccussed in the [next section](#choice-of-priors)) with unknown mean $\theta_{\mu}$ and standard deviation $\theta_{\sigma}$.

In the case of separate models, we assume that each team has its own model and so we will build separate models which will contain no other information other than the performances of the teams on their matches. To implement that, we use separate priors $\theta_j$ and $\sigma_j$ for each of the teams.



## Choice of Priors

Our first trials consisted of trivial priors which contain no problem-specific information, meaning that we did not use any current knowledge while picking them. The reason we chose these weakly informative priors is because we did not want to use current knowledge since it would skew our results towards a certain assumption (e.g. that the teams of region $X$ perform better) which would not hold in the general case.

While tuning these priors we noticed that the results and the output diagnostics were mostly the same. This made us stay with the choices above since these weakly informative priors showed some stability. 

In our problem we represent our observed performances by $y_{ij}$, where $i=1..N$ denotes the current match and $j$ corresponds to the team of a certain region. So $y_{ij}$ is the performance score of team j in game i. We can now mathematically express the separate model as:
$$
y_{ij} \sim N(\mu_j, \sigma_j)
$$
$$
\mu_j \sim N(0, 100)
$$
$$
\sigma_j \sim Inv-\chi^2(0.1)
$$


For the hierarchical model these weakly informative priors can be mathematically expressed as follows:

$$
y_{ij} \sim N(\theta_j, \sigma)
$$

$$
\theta_{\mu} \sim N(0, 100)
$$
$$
\theta_{\sigma} \sim inv-\chi^2(0.1)
$$
$$
\theta_j \sim N(\theta_{\mu}, \theta_{\sigma})
$$
$$
\sigma \sim inv-\chi^2(0.1)
$$


In the representations $\theta_{\mu}$ and $\theta_{\sigma}$ are the hyper-priors of the means (and the means are denoted as $\theta_j$ for each team).


## Separate Model

Firstly, we built a separate model which could reflect each team performance separately. The idea behind the hierarchical model has already been discussed and so we are going to go straight to the implementation.



### Using the model
You may find the stan implementation of the separate model in the [appendix (Stan Code)](#separate-model-impementation). Now in order to run the separate model on our dataset we are using the following R code:

```{r, results='hide', message = FALSE, warning=FALSE}
data <- read.csv(file = './data/match_performance_group_region.csv')  # read dataset

sm <- rstan::stan_model(file = './model/Separate.stan')  # load model
stan_data <- list(y = data,  # team performances
                  N = nrow(data),  # number of matches of each team
                  J = ncol(data))  # number of teams
model_sm <- rstan::sampling(sm, data = stan_data) # load data to our model and compile

draws_sm <- as.data.frame(model_sm)  # convert to data frame
```

## Hierarchical Model

The goal here is to create a hierarchical model for the teams of each region. As we have already mentioned above, we can divide the 18 teams to six regions (Europe, China, CIS, Southeast Asia, North America, South America).

Now, in order to build the models, we are going to use the same hierarchical structure for all different regions. This way we are sure that there is no external bias. 

### Using the Model
You may find the stan implementation of the hierarchical model in the [appendix (Stan Code)](#hierarchical-model-impementation). Here, for each region, we run the hierarchical model on its data set points (the matches of the teams of that region). For example, in the Europe region, we have the team Alliance, CHAOS, OG, NiP, Secret, Liquid to form the data set. 

```{r,results='hide',message=FALSE,warning=FALSE}
hm <- rstan::stan_model(file = './model/Region-Hierarchical.stan')
Europe_data <- list(y = data[,1:6],
                  N = nrow(data[,1:6]),
                  J = ncol(data[,1:6]))
model_Europe_hm <- rstan::sampling(hm, data = Europe_data)
model_Europe_hm
draws_Europe_hm <- as.data.frame(model_Europe_hm)
```

# Convergence Diagnostics


## Separate Model


## Hierarchical Model

With the above weakly informative priors we managed to reach converge after compiling 5 stan models (one for each region) as follows (only for the european region):

```{r,results='hide',message=FALSE,warning=FALSE}
hm <- rstan::stan_model(file = './model/Region-Hierarchical.stan')
Europe_data <- list(y = data[,1:6],  # the observed performances
                    N = nrow(data[,1:6]),  # number of teams
                    J = ncol(data[,1:6]))  # number of matches
# compile the model with a standard seed
model_Europe_hm <- rstan::sampling(hm, data = Europe_data, seed=42)
draws_Europe_hm <- as.data.frame(model_Europe_hm)
```

# Posterior Predictive Checks
We compare the MCMC results with the performance data which we used.


# Model Comparison

# Predictive Performance Assessment

## Predictive Performance
The prediction workflow for a series match between team $X$ and $Y$ is the following: 

1. A normal series is a two out of three game series. But here we only compare each team's expected performance scores to predict who is more likely to win in the series.

2. The $ypred$ in stan code represents the predict performance score of each team. So we compare the $ypred$ of team $X$ and $Y$, we could get the win possibility. The function predict_result_sm below will do this:
```{r}
predict_result_sm <- function(team_num_1, team_num_2) {
  ypred_1 <- draws_sm[, 36 + team_num_1]
  ypred_2 <- draws_sm[, 36 + team_num_2]
  win_rate_1 = {}
  for (i in 1:length(ypred_1)) {
    win_rate_1[i] = ypred_1[i] / (ypred_1[i] + ypred_2[i])
  }
  
  mean_rate_1 <- (mean(win_rate_1))
  
  return(mean_rate_1)
}
```

3. If the win possibility of team $X$ is above 0.5, we will say that the team $X$ is more likely to win in this game series.

Here is a example to predict which team will win between PSG.LGD and VP.
```{r}
#return the win team
compare <- function(team_name_1,team_name_2){
  team_num_1 <- which(colnames(data) == team_name_1)
  team_num_2 <- which(colnames(data) == team_name_2)
  predict <- predict_result_sm(team_num_1,team_num_2)
  if(predict>=0.5)
    return(team_name_1)
  else 
    return(team_name_2)
}

#Calculate the result of main event stage
compare("PSG.LGD","VP")
```

We actually apply this method to all the series in main event. Here is the predict result compare to the real one. The green boxes indicate that the predictions correspond to the actual results, red boxes indicate that the predictions do not correspond to the actual results.

Predictive Performance of separate model: 

![separate_result](./pictures/separate_result.png)

Predictive Performance of hierarchical model:  

![hierarchical_result](./pictures/hierarchical_result.png)

## Analysis
We could find there is no significant difference in the results between separate model and hierarchical model. It shows that regional factors do not play a significant role. In other words, there are no significant differences between the teams in the different regions.

And the prediction accuracy is not very high.The reasons could be the following:

1. We just predict the main event result based on group stage performance. It often happens that a team is in poor form in the group stage, but gets better and better in the knockout stages. With this type of team, our predictions are difficult to be accurate. Like the Team Liquid, it was only 14th out of all teams in the group stage, but in the end it finished in second place. So we were almost entirely wrong in our predictions for this team.

2. Maybe our performance score could not completely reflects the power of each team. We will discuss it in more detail in improvement part.

# Sensitivity Analysis

# Discussion of Issues and Possible Improvements
From the above results, we can see that there are still a number of issues improvements can be made in the current work.

1. The data from the group stage matches does not fully reflect the ability of the teams, so we should also include the previous performance of each team in the database.

2. The method of adding up all the parameters to get a performance score is too simple. Perhaps we should find a more accurate way of representing the performance of the individual teams. For example, by assigning different weights to performance indicators based on group stage ranking, so that the final score matches their actually performance.
# Conclusion

# Self-Reflection

## Rongzhi Liu
There are many things in life that cannot be accurately and simply measured by a number, such as how much he loves me. This work made me realize that it is sometimes more difficult to get the right data than it is to analyse it using Bayesian theory. In practice, there is a wide range of knowledge to be used and a variety of problems to be encountered. The practical problems often do not follow the fixed processes that we learn about on the course. For different problems, we first need to clarify the structure and requirements for solving them. If the structure is clear and precise, even a very simple model can give satisfactory results.

# Appendix

## Data Transformation

Below, we present the python functions that were used for data preprocessing and for computing the performance for a single team of a single match.

```python
def _score_gap(match):
    # Simple score difference
    score = match[['radiant_score', 'dire_score']].values[0]
    return score[0] - score[1]
    
def _xp_gap(match, use_weights=True):
    # Take the sum of the xp advantages (over all minutes)
    xp_list = np.array(ast.literal_eval(pd.unique(match.radiant_xp_adv)[0]))
    # A negative sum would mean that the radiant team was on a disadvantage 
    # most of the times
    if use_weights:
        # The last few minutes are more important and so we should give a 
        # greater weight to these
        weights = np.linspace(0.1, 1, num=len(xp_list))  # avoid zero weights
        xp_list_weighted = weights * xp_list
        return np.sum(xp_list_weighted)
    return np.sum(xp_list)

def _gold_advantage(match, use_weights=True):
    gold_list = np.array(ast.literal_eval(pd.unique(match.radiant_gold_adv)[0]))
    # A negative sum would mean that the radiant team had less gold for 
    # most of the game
    if use_weights:
        weights = np.linspace(0.1, 1, num=len(gold_list))
        gold_list_weighted = weights * gold_list
        return np.sum(gold_list_weighted)
    return np.sum(xp_list)
    
def _hero_specific_scores_v2(match):  # hero damage and hero healing
    stats = {
        "damage": match['hero_damage'].values, 
        "healing": match['hero_healing'].values,
        "kda": match['kda'].values,
        "wards": match['obs_placed'].values,
    }  # will contain the damage and healing scores for the radiant players
    # Simply use the sum
    dmg_diff = np.sum(stats['damage'])
    heal_diff = np.sum(stats['healing'])
    kda_diff = np.sum(stats['kda'])
    wards_diff = np.sum(stats['wards'])
    return dmg_diff, heal_diff, kda_diff, wards_diff
    
def performance(match):
    # match should be a dataframe that contains 5 rows 
    #    (a match entry for a certain team)
    score_gap = _score_gap(match)
    xp_gap = _xp_gap(match, use_weights=True)
    gold_adv = _gold_advantage(match, use_weights=True)
    # case 3 (simplistic approach)
    total_dmg, total_heal, total_kda, total_wards = _hero_specific_scores_v2(match)  
    return score_gap, xp_gap, gold_adv, total_dmg, total_heal, total_kda, total_wards
```

By using the performance function above for both teams of every match we can get a new dataset which will contain all of the $7$ characteristics returned by $performance$. We can then normalize these values with the method mentioned in the [Data and Preprocessing](#data-and-preprocessing) section and then simply sum the values of each row in order to get the final performance score.



## Stan code

### Separate Model Implementation

```{stan output.var="Separate.stan", warning=FALSE}
data {
  int < lower =0> N;//the number of matches
  int < lower =0> J;//the number of teams 
  vector [J] y[N];//the performance score
}

parameters {
  vector [J] mu;
  vector < lower =0 >[J] sigma ;
}

model {
  for (j in 1:J){
    mu[j] ~ normal (0, 10);
    sigma [j] ~ inv_chi_square (0.1);
  }

  // likelihood
  for (j in 1:J)
    y[,j] ~ normal (mu[j], sigma [j]);
}

generated quantities {
  vector [J] ypred;
  // Compute predictive performance for each team
  for (j in 1:J)
    ypred[j] = normal_rng (mu [j] , sigma [j]);
}
```

### Hierarchical Model Implementation
```{stan output.var="Region-Hierarchical.stan",warning=FALSE}
data {
  int < lower =0> N;//the number of matches
  int < lower =0> J;//the number of teams in this region
  vector [J] y[N];//the performance score
}

parameters {
  real mu_p;
  real < lower =0 > sigma_p;
  vector [J] theta;
  real < lower =0 > sigma ;
}

model {
  // priors
  mu_p ~ normal (0, 10);
  sigma_p ~ inv_chi_square (0.1);
  
  for (j in 1:J){
    theta[j] ~ normal (mu_p, sigma_p);
  }
  sigma ~ inv_chi_square (0.1);

  // likelihood
  for (j in 1:J)
    y[,j] ~ normal (theta[j], sigma);
}

generated quantities {
  vector [J] ypred;
  // Compute predictive performance for each team in this region
  for (j in 1:J)
    ypred[j] = normal_rng (theta[j] , sigma);
}
```
# References
